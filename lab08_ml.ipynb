{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of selected root node: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    \"\"\"Calculate the entropy of a set of labels.\"\"\"\n",
    "    # Count the occurrences of each label\n",
    "    label_counts = {}\n",
    "    total_samples = len(labels)\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    \n",
    "    # Calculate entropy using the formula: -p_i * log2(p_i)\n",
    "    entropy = 0\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(feature_values, labels):\n",
    "    \"\"\"Calculate the information gain for a feature.\"\"\"\n",
    "    total_samples = len(labels)\n",
    "    parent_entropy = calculate_entropy(labels)\n",
    "    unique_values = set(feature_values)\n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    # Calculate the entropy of each subset based on unique feature values\n",
    "    for value in unique_values:\n",
    "        subset_indices = [i for i, val in enumerate(feature_values) if val == value]\n",
    "        subset_labels = [labels[i] for i in subset_indices]\n",
    "        subset_entropy = calculate_entropy(subset_labels)\n",
    "        subset_weight = len(subset_indices) / total_samples\n",
    "        weighted_entropy += subset_weight * subset_entropy\n",
    "    \n",
    "    # Calculate information gain as the difference between parent entropy and weighted entropy\n",
    "    information_gain = parent_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def select_root_node(features, labels):\n",
    "    \"\"\"Select the root node using Information Gain.\"\"\"\n",
    "    num_features = len(features[0])\n",
    "    information_gains = []\n",
    "    \n",
    "    # Calculate information gain for each feature and select the one with maximum gain\n",
    "    for i in range(num_features):\n",
    "        feature_values = [sample[i] for sample in features]\n",
    "        information_gain = calculate_information_gain(feature_values, labels)\n",
    "        information_gains.append(information_gain)\n",
    "    \n",
    "    # Select the index of the feature with maximum information gain\n",
    "    root_node_index = np.argmax(information_gains)\n",
    "    return root_node_index\n",
    "\n",
    "features = [[1, 0], [1, 1], [0, 0], [0, 1]]\n",
    "labels = [1, 1, 0, 0]\n",
    "root_node_index = select_root_node(features, labels)\n",
    "print(\"Index of selected root node:\", root_node_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Width Binned Values: [0, 0, 1, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def equal_width_binning(feature_values, num_bins):\n",
    "    \"\"\"Perform equal width binning for continuous-valued features.\"\"\"\n",
    "    # Calculate the bin width based on the range of feature values\n",
    "    min_value = min(feature_values)\n",
    "    max_value = max(feature_values)\n",
    "    bin_width = (max_value - min_value) / num_bins\n",
    "    \n",
    "    # Assign each feature value to the corresponding bin\n",
    "    binned_values = [int((value - min_value) // bin_width) for value in feature_values]\n",
    "    return binned_values\n",
    "\n",
    "def frequency_binning(feature_values, num_bins):\n",
    "    \"\"\"Perform frequency binning for continuous-valued features.\"\"\"\n",
    "    # Count the occurrences of each feature value\n",
    "    value_counts = Counter(feature_values)\n",
    "    \n",
    "    # Calculate bin counts based on the frequency of feature values\n",
    "    bin_counts = {value: i // (len(value_counts) // num_bins) for i, value in enumerate(value_counts.keys())}\n",
    "    \n",
    "    # Assign each feature value to the corresponding bin\n",
    "    binned_values = [bin_counts[value] for value in feature_values]\n",
    "    return binned_values\n",
    "\n",
    "def bin_continuous_feature(feature_values, num_bins, binning_type='equal_width'):\n",
    "    \"\"\"Bin continuous-valued features using the specified binning type.\"\"\"\n",
    "    if binning_type == 'equal_width':\n",
    "        return equal_width_binning(feature_values, num_bins)\n",
    "    elif binning_type == 'frequency':\n",
    "        return frequency_binning(feature_values, num_bins)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning type. Choose either 'equal_width' or 'frequency'.\")\n",
    "\n",
    "feature_values = [2.5, 3.5, 5.0, 6.5, 7.2, 8.0]\n",
    "num_bins = 3\n",
    "binning_type = 'equal_width'\n",
    "binned_values = bin_continuous_feature(feature_values, num_bins, binning_type)\n",
    "print(\"Equal Width Binned Values:\", binned_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 <= 1.5\n",
      "  Leaf: Class 0\n",
      "  Feature 0 <= 6.5\n",
      "    Leaf: Class 1\n",
      "    Feature 0 <= 7.5\n",
      "      Leaf: Class 0\n",
      "      Leaf: Class 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feat=None, split=None, left=None, right=None, leaf=None):\n",
    "        \"\"\"Initialize a tree node.\"\"\"\n",
    "        self.f = feat    # Feature index for splitting\n",
    "        self.s = split   # Split value for continuous features\n",
    "        self.l = left    # Left child node\n",
    "        self.r = right   # Right child node\n",
    "        self.leaf = leaf # Leaf node value (class label if leaf)\n",
    "    def print_tree(self, depth=0):\n",
    "        \"\"\"Print the tree structure recursively.\"\"\"\n",
    "        indent = \"  \" * depth\n",
    "        if self.leaf is not None:\n",
    "            print(indent + \"Leaf: Class\", self.leaf)\n",
    "        else:\n",
    "            print(indent + \"Feature\", self.f, \"<=\", self.s)\n",
    "            if self.l:\n",
    "                self.l.print_tree(depth + 1)\n",
    "            if self.r:\n",
    "                self.r.print_tree(depth + 1)\n",
    "def entropy(labels):\n",
    "    \"\"\"Calculate entropy of labels.\"\"\"\n",
    "    counts = {lbl: labels.count(lbl) for lbl in labels}  # Count occurrences of each label\n",
    "    total = len(labels)                                  # Total number of labels\n",
    "    entropy = sum(-count/total * np.log2(count/total) for count in counts.values())  # Calculate entropy\n",
    "    return entropy\n",
    "\n",
    "def best_split(features, labels):\n",
    "    \"\"\"Find best feature and split value.\"\"\"\n",
    "    best_gain = 0       # Initialize best information gain\n",
    "    best_feat = None    # Initialize best feature index\n",
    "    best_split = None   # Initialize best split value\n",
    "    for i in range(len(features[0])):  # Iterate over each feature\n",
    "        values = [sample[i] for sample in features]  # Get feature values\n",
    "        unique = sorted(set(values))   # Get unique feature values\n",
    "        for j in range(len(unique) - 1):  # Iterate over unique feature values for possible splits\n",
    "            split = (unique[j] + unique[j+1]) / 2  # Calculate split value as average of adjacent unique values\n",
    "            left_lbls = [labels[k] for k in range(len(labels)) if features[k][i] <= split]  # Labels in left split\n",
    "            right_lbls = [labels[k] for k in range(len(labels)) if features[k][i] > split]   # Labels in right split\n",
    "            # Calculate information gain\n",
    "            gain = entropy(labels) - (len(left_lbls)/len(labels)) * entropy(left_lbls) - (len(right_lbls)/len(labels)) * entropy(right_lbls)\n",
    "            # Update best gain, feature index, and split value if current gain is higher\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feat = i\n",
    "                best_split = split\n",
    "    return best_feat, best_split\n",
    "\n",
    "def build_tree(features, labels, max_depth=None):\n",
    "    \"\"\"Build decision tree recursively.\"\"\"\n",
    "    # Base case: if maximum depth is reached or all labels are the same\n",
    "    if max_depth is not None and (max_depth == 0 or len(set(labels)) == 1):\n",
    "        return Node(leaf=max(labels, key=labels.count))  # Create leaf node with most common label\n",
    "    feat, split = best_split(features, labels)  # Find best feature and split value\n",
    "    # If no best split is found (e.g., due to categorical features with the same values), create leaf node\n",
    "    if feat is None:\n",
    "        return Node(leaf=max(labels, key=labels.count))\n",
    "    # Partition data based on best split\n",
    "    left_feats = [s for s in features if s[feat] <= split]  # Features in left split\n",
    "    left_lbls = [labels[i] for i in range(len(labels)) if features[i][feat] <= split]  # Labels in left split\n",
    "    right_feats = [s for s in features if s[feat] > split]  # Features in right split\n",
    "    right_lbls = [labels[i] for i in range(len(labels)) if features[i][feat] > split]  # Labels in right split\n",
    "    # Recursively build left and right subtrees\n",
    "    left = build_tree(left_feats, left_lbls, max_depth - 1 if max_depth is not None else None)\n",
    "    right = build_tree(right_feats, right_lbls, max_depth - 1 if max_depth is not None else None)\n",
    "    return Node(feat=feat, split=split, left=left, right=right)  # Create internal node with best split\n",
    "\n",
    "features = [[5, 1], [3, 1], [8, 2], [2, 2], [6, 3], [7, 3]]  \n",
    "labels = [0, 0, 1, 1, 1, 0] \n",
    "tree = build_tree(features, labels)  \n",
    "tree.print_tree()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
